\documentclass{vldb}
\usepackage{graphicx}
\usepackage{balance} 

\begin{document}


\section{Introduction}

\par The International Movie Database (IMDb) with its over 3 million listed titles provides a great resources for information about all sorts of movies. Maintaining this amount of data is quite challenging as new movies need to be added all the time.
\par Although the data is in a multi GB space one would not necessarily consider it to be ‘Big Data’. Especially in comparison to things like Twitters continuous stream of data or even large scale sensor networks the IMDb is quite small. Any analysis could probably be done with conventional systems. But for this academic project we wanted to try to build a scaleable solution, which could be easily adopted for other, larger data sources.

\par For this project we decided to try a classification task on the available movie metadata to predict its genres. This could be helpful for people who enter information about new movies or also to complete the IMDb since there are a lot of movies without genre information.
Part of this analysis is text classification which is a problem that occurs in many domains.

\par For the genre classification it is also interesting whether it is possible to predict a genre in a reliable way. Often this task is challenging even for humans cause a genre itself doesn’t have formal specification and can be a bit fuzzy. There are also mixups of genres, like a tragic comedy, which might be hard to classify since they contain features of multiple genres. Evaluating whether a machine can learn sort of a formal definition in form of some model is interesting since a further analysis of the model could lead to a formal, human understandable definition of a genre. It could solve the question of “What makes up a genre?”.



\section{Problem Statement}
\subsection{Background}
Movie genres are manually created categorical labels to characterize different movies. These genre labels can be used to describe the movie, contribute in movie recommendation based on the domain specified interests and some other fields. Currently the movie genres annotation are determined by human beings. The complexity in automatic genre classification is caused by the fact that usually the genres are not well  defined. Normally there are no strict definitions or boundaries between 2 different genres.  And of cause they share certain overlapping characteristics.
\par Facing the fact that the number of movie is growing at a fast pace, automatic movie genres classification can assist or replace the human work in this process and provide important component for the complete movie information and in addition, structure the movie based on the genres. Thus, we begin to think about: what kind of information is sufficient to automatically classify the movie and how to approach that?
\par Thus, our project is aimed to building a scalable movie genre classification system using Internet Movie Database(IMDb) and compare the results using different features like plot description, actor and key words. 

\subsection{Dataset} 
The dataset we use is the publicly available data from the Internet Movie Database(IMDb). IMDb is an online dataset which provides all kinds of information related to movies. The information includes actors, production crew, fictional characters featured in these and so on. There are some publicly available datasets containing a subset of the online database, which can be downloaded and processed. 
\par The dataset is organized into a set of compressed text files, where each file contains information on a specific aspect. The 4 main files we use are genres.list, plot.list, actors.list and keywords.list. The size of the total files is about 1.7GB. Although is not big enough to be considered as "big data", but it is easy to adopted for larger data.
\subsection{Tools}
\par To choose suitable tools, we need to meet the following requirements:
\begin{itemize}
\item {Easy and convinient way to preprocess the raw data to get formatted data}
\item {Scalable data process framwork with suited machine learning algorithms}
\end{itemize}
\par By comparing the advantages and disadvantaged, we deceide to use these tools:
\par Python: Python is a fast and friendly language. It’s easy to learn, can work everywhere and take much less time to develop. Python is an excellent tool for scanning and manipulating textual data. The raw data from IMDb can not be used directly, so we choose Python to do the preprocessing work and generate processed data for the next steps. 
\par Apache Spark: Spark is an open source Apache project built around speed, ease of use. It provides a faster and more general data processing platform. Spark enables we to quickly write applications in Java, Scala, or Python. It comes with a built-in set of over 80 high-level operators. Other than Spark Core API, there are additional libraries that are part of the Spark ecosystem and provide additional capabilities in big data analytics and machine learning areas. The scalable machine learning library consisting of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, as well as underlying optimization primitives.

\section{Methodology}
\subsection{Apache Spark Pipeline}
In machine learning, it is common to run a sequence of algorithms to process and learn from the data, a regular text document processing workflow might include several steps:
\begin{itemize}
\item {Split the document into words.}
\item {Convert each document's words into a numerical feature vector.}
\item {Learn a prediction model using the feature vectors and labels.}
\end{itemize}
\par Spark ML represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (each stage is either a Transformer or an Estimator) to be run in a specific order. For Transformer stages, the transform() method is called on the dataset and he input dataset is modified as it passes through each stage.   For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline), and that Transformer’s transform() method is then called on the dataset.
\par Figure 1 shows the 3-stage pipeline we used in the project. These 3 stages are all transformers.  Each stage’s transform() method updates the dataset and passes it to the next stage. The bottom row represents data flowing through the pipeline, where cylinders indicate DataFrames. The Tokenizer.transform() method splits the raw text documents into words, adding a new column with words into the dataset. The feature extraction stage converts the words column into feature vectors, adding a new column with those vectors to the dataset. Then SVM.train() is called to produce a SVMModel. After that the pipeline calls SVM.predict() method on the dataset and predict the result.
\begin{figure}
\begin{center}
\includegraphics[width=3.50in]{pipeline.png}
\caption{movie genres classification pipeline}
\end{center}
\end{figure}
\par In feature extraction stage, we use different feature extraction methods for the 3 different input data (plot, actors and key words). For plot input, HashingTF() and IDF() is applied to generate the feature vectors. For the rest 2 kinds of input data, since there are no duplicated words, we simply use binary n-vectors to represent whether a actor(or key word) appears.
\subsection{Preprocessing}
\par To get start, we first use Tableau to get a insight of the dataset and see what genres we are deeling with. Figure 2 shows the contribution of all the movie genres. We can easily see that drama, comedy and documentary have the maximum number of movies, almost 3 times of the rest genres. This means for those 3 genres, there are much more input data to train the corresponding SVM models. This may increase the classification accuracy. On the other hand, the large quantity means the scope for these 3 genres are much larger, which means are not much common similarity inside each scope, leading to decrease the accuracy.
\begin{figure}
\begin{center}
\includegraphics[width=3.00in]{genresContribution.png}
\caption{insight of the data using Tableau}
\end{center}
\end{figure} 
\par Another part of preprocessing is to scan the raw data and get the formatted data input for the Spark pipeline. This is done by a few lines of Python script.
\subsection{Feature Extraction}
\par In the process of plot description, we use TF-IDF to vecterize the feature.
\par Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method that is intended to reflect how important a word is to a document in a collection of the corpus. The tf-idf value increases probability to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general but provides few information.
\par Denote a term by $t$, a document by $d$, and the corpus by $D$. Term frequency $TF(t,d)$ is the number of times term $t$ appears in document $d$, while the document frequency $DF(t,D)$ is the number of documents that contain term $t$ in corpus $D$. However, if we only use term frequency, for some common words, like "the", it will tend to incorrectly emphasize the documents which happen to use "the" more frequently. But these kind of words can't provide enough information to distinguish relevant and non-relevant documents and terms. If a term appears very often across the corpus, it means it doesn’t carry special information about a particular document. Thus, we use inverse document frequency, a numerical measure to show how much information a term carries:
\begin{displaymath}
IDF(t,D) = \log\frac{|D|+1}{DF(t,D)+1},
\end{displaymath}
where $|D|$ is the number of documents in the corpus. Since logarithm is used, if a term appears in all documents, its $IDF$ value becomes 0. Note that a smoothing term is applied to avoid dividing by zero for terms outside the corpus. The TF-IDF measure is simply the product of $TF$ and $IDF$:
\begin{displaymath}
TFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).
\end{displaymath} 
\par Also, in the implementation of TF-IDF of in  Apache Spark machine learning library, raw feature is mapped into an index(term) by using a hashing function. Then term frequency is calculated based on the mapped indices. This approach avoids the need to compute a global term-to-index map, which can be expensive for a large corpus, but it suffers from potential hash collisions, where different raw features may become the same term after hashing. To reduce the chance of collision, we can increase the target feature dimension, i.e., the number of buckets of the hash table. The default feature dimension is $2^{20}=1048576$
\par By using tf-idf, we generate a vector for every plot description.

\subsection{Classification}
In machine learning, support vector machines(SVMs) are supervised learning models used for binary classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. 
\par An SVM model constructs a hyperplane in high(or infinite)dimensional space, which can be used for classification, regression, or other tasks. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.
\par Given some training dataset $\mathcal{D}$ containing $n$ points：
\begin{displaymath}
\mathcal{D} = \{(\mathbf{x}_i,y_i)|\mathbf{x}_i \in \mathbb{R}^p,y_i\in\{-1,1\}\}^n _{i=1}
\end{displaymath}
where $y_i$ is either $1$ or $-1$, indicating the class to which $\mathbf{x}_i$ belongs. Each $\mathbf{x}_i$ is a $p$ dimentional vector. If the data is linealy seperable, we can find a hyperplane:
\begin{displaymath}
\mathbf{w}\cdot\mathbf{x}-b=0 
\end{displaymath}
to seperate the dataset and by determin the distance from the data point to the hyperplane $|\mathbf{w}\cdot\mathbf{x}+b|$ is positive or negative to see which class this pint belongs to.

\par  To find the maximum margin to divide the dataset into 2 classes($y=1$ or $y=-1$), we can select two hyperplanes in a way that they separate the data and there are no points between them, and then try to maximize their distance. The region bounded by them is called "the margin". These hyperplanes can be described by the equations
\begin{displaymath}
\mathbf{w}\cdot\mathbf{x}-b=1
\end{displaymath}
and
\begin{displaymath}
\mathbf{w}\cdot\mathbf{x}-b=-1
\end{displaymath}
\begin{figure}
\begin{center}
\includegraphics[width=2.50in]{SVM.jpeg}
\caption{margins for an SVM trained with samples from two classes}
\end{center}
\end{figure}
\par By using geometry, we find the distance between these two hyperplanes is $\frac{2}{||w||}$, so we want to minimize $||w||$. As we also have to prevent data points from falling into the margin, we add the following constraint:
\begin{displaymath}
\begin{cases}
\mathbf{w}\cdot\mathbf{x}-b\ge1,y_i=1\\
\mathbf{w}\cdot\mathbf{x}-b\le-1,y_i=-1\\
\end{cases}
\end{displaymath}
This can be written as:
\begin{displaymath}
y_i(\mathbf{w}\cdot\mathbf{x}_i-b)\ge1,1\le i \le n
\end{displaymath}
\par More clearly, the constraint problem can be written as:
\begin{displaymath}
\arg\min\frac{1}{2}||\mathbf{w}||^2
\end{displaymath}
subject to (for any $i = 1,...,n$)
\begin{displaymath}
y_i(\mathbf{w}\cdot\mathbf{x}_i-b)\ge1
\end{displaymath}
\par After getting the $\mathbf{w}$, given a new data point, denoted by $\mathbf{x}$, the model makes predictions based on the value of $\mathbf{w}\mathbf{x}$. By the defalut, if $\mathbf{w}\mathbf{x} \ge 0$, the outcome is positive, otherwise negative. 
\par For all the 24 genres, we need to build a SVM model for every genre and predict every genre seperately.


\section{Experiment}

% TODO: add graph from slides ?

\par We started our experiments with using the plot descriptions and performed multiple runs with different number of learning iterations of the SVMs. We started with 5 training iterations and increased that number by 5 until we reached a maximum of 100 training iterations. For every training and testing run 70 \% of the data was used for training and the remaining 30 \% were used for testing to see how good the learned models classify unseen data. After the plot descriptions we used other features of the movie to see whether we could do better. We also combined several features.

\par The first approach was to use the plot description and transform it to a TF-IDF vector for each movie. For this approach we could use 249761 movies, which had a plot description as well as a genre label. We also tried to tune the IDF calculation by setting a min-doc-frequency. With this we could exclude words which do not appear in a minimum number of documents, but this seemed to not affect our results significantly so we didn't perform a detailed analysis of the effects of this parameter.

\par The second approach was to use keywords of the movies, which were explicitly assigned by the maintainers of the IMDb. This provides us a similar textual representation of the content of the movie but with the advantage of being already filtered by a human and also containing words that do not appear in a typical plot description but might describe the type of  the movie quite well. We didn’t perform a TF-IDF transformation here and just constructed a vector per movie containing a 1 if it has a certain keyword and 0 if not. With this approach we could use 395647 movies, which provided a set of keywords and a genre label.

\par The third approach was to create a connection between actors and the genre of movies they usually appear in. This feature isn’t content based at all but could be of significance cause we assume that most actors stick to one kind of genre during their carer. Similar to the keywords approach we didn’t use a TF-IDF transformation and just binary n-dimensional vector to represent the fact whether a actor appeared in the movie or not. Here we had a dataset of 693520 movies, which provided a list of actors as well as a genre label. We only tested with the actors data and not on with the actress data as well, cause our results with this feature wasn't that satisfying and we decided to spend the time on other research directions.

\par As a final experiment we combined all 3 features. Therefore we constructed a set of 3 feature vectors - one for plot, one for keywords and one for actors - per movie. Now we trained 3 SVMs per genre - one for each feature vector. In the testing phase we feed the feature vectors to the SVMs and build a simple average over the results of all 3 SVMs per genre. Cause we could only use movies which provide valid feature vectors for all 3 features, we could only use a dataset of 103536 movies.

\par See Figure \ref{fig:FeaturesAndGenresBarChart} for a comparison of the dataset sizes we were able to use for the different features.

\par For all experiments we tried to classify the movie in 24 genres - see Table \ref{genre-list}. We didn’t consider the genre “Short” in our computation and analysis since it’s a very large and fuzzy genre which is basically only based on the length of a movie but the goal was to create a content related classification. Also the genres Lifestyle, Erotica, Experimental, Commercial \& Film-Noir weren't used cause they only contained a very small number of movies - sometimes only one. The genres Game-Show and Talk-Show weren't used cause they are not classical movies in the sense of having a plot, professional actors etc..

\begin{table}[]
\centering
\caption{List of genres used for classification and their number of movies}
\label{genre-list}
\begin{tabular}{lllll}
Action & 61198 & Adult & 68936\\
Adventure & 37974 & Animation & 52849\\
Biography & 24995 & Comedy & 235054\\
Crime & 43830 & Documentary & 207193\\
Drama & 317577 & Family & 50336\\
Fantasy & 33063 & History & 21277\\
Horror & 44884 & Music & 43939\\
Musical & 16848 & Mystery & 28307\\
News & 12386 & Reality-TV & 13327\\
Romance & 60534 & Sci-Fi & 29680\\
Sport & 20316 & Thriller & 59793\\
War & 14992 & Western & 14722\\
\end{tabular}
\end{table}


\begin{figure}
\begin{center}
\includegraphics[width=3.00in]{FeaturesAndGenresBarChart.png}
\caption{number of movies with a certain feature and a genre labeling}
\label{fig:FeaturesAndGenresBarChart}
\end{center}
\end{figure}


\section{Results}

\begin{figure}
\begin{center}
\includegraphics[width=3.00in]{CCCM.png}
\caption{completely correctly classified movies based on different features and different number of SVM traning iterations}
\label{fig:CCCM}
\end{center}
\end{figure}

\subsection{Plot Description Based}
\par Using the plot description (see Figure \ref{fig:ErrorRatesPlot} for error rate graphs) as the only feature wasn’t really successful. Although the portion of erroneous single classification operations was at around 6 \%, there were many movies which weren’t classified correctly. This is caused by the fact that we use 24 SVMs - one SVM for each genre. Most of these SVMs answer with a correct prediction - in most cases with “false”, cause a movie usually only belongs to 1 to 3 genres. But it is enough that one of those SVMs answers with a wrong value and the movie ends up misclassified.
\par Another interesting observation is that the amount of errors for the genres Drama, Comedy and Documentary is significantly higher than for other genres. Those 3 genres are the most common ones. One could expect that the SVMs are trained more precisely cause of a larger amount of positive training examples and a therefore more even distribution of negative and positive data points. But it seems like that the larger amount of movies in those genres leads to more diverse plot descriptions and a more fuzzy definition of the genre.
\par When looking at the development of false-positive and false-negative classifications over increasing number of training iterations, one can see a strong oscillation of the model for Drama. With 5 iterations the false-positive rate is low and the false-negative rate is high, for 20 iterations the false-positive rate is high and the false-negative rate is low. The gap and error rate reduces with increasing number of training iterations but continues to be visible even at around 100 iterations. A similar pattern can be sen for the Comedy genre, although the oscillation isn't that distinctive.
\par For most of the other genres false-positives aren’t an issue until 10 to 15 or even until 35 iterations. This is most likely due to the small amount of positive training examples which leads to strong tendency of always answering with no.
\par The false-negative rates are quite stable with in most cases a slight tendency to a lower error rate, but there are also some genres like Romance which have a constant increase in the error rate with increasing number of iterations. The most significant improvements can be seen between 0 and 25 iterations.
\par If we look at the overall error rate there’s usually an improvement with increasing number of training iterations. It seems to be stable after around 50 iterations. An reduction of false-negatives is often consumed by an increase in false-positives, leading to nearly horizontal lines for some genres. The alternating false-negatives and false-positives for genres like Drama or Comedy even themselves out and lead to a continuous curve in the total error rate.
\par The number of completely correctly classified movies increased nearly continuously with increasing number of iterations - as one would expect it to be after seeing decreasing error rates. Interestingly there are small drops in the curve at 15 and 25 iterations. Those are the same spots where the Drama false-negative rates are high but the Drama false-positive rates are low. The curve increases significantly until 30 iterations with 23.7 \% accuracy and only slowly develops to 25 \% accuracy at 100 iterations.
\par Overall the approach wasn’t satisfying with only ~25 \% completely correctly classified movies and in general the following rules hold:
\begin{itemize}
\item {The more SVMs iterations the more precise is the classification.}
\item {The more movies belong to a genre the more errors in classification will happen.}
\item {For reasonable results one can stop training after around 50 iterations.}
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[width=3.00in]{ErrorRatesPlot.png}
\caption{error rates for selected genres using plot descriptions as the only feature}
\label{fig:ErrorRatesPlot}
\end{center}
\end{figure}

\subsection{Keyword Based}
\par In the next experiment (see Figure \ref{fig:ErrorRatesKeywords})  we used the keywords as the only feature. The overall result was slightly better with 27.8 \% to 29.6 \% completely correctly classified movies.
As for the plot based classification the genres Drama, Comedy and Documentary were the most difficult  ones to classify. With false-positive rates of 13 \% to 19 \%. All other genres had false-positive rates below 7 \%. 
The false-positive rate for Comedy was higher than for all other genres. Interestingly the false negative rates were all below 2 \%, except for Drama which was between 10 \% and 11 \%. Nevertheless those rates were much better than for the plot based classification approach. For a higher number of iterations the same issue if increasing false-negatives rates were observable. The phenomena of alternating high and low false-negative and false-positive rates couldn’t be observed in this experiment.
\par In the overall error rates Drama was the most difficult to classify (~24 \% error) followed by Comedy (~19 \%) and Documentary(~12 \%). All other genres where quite close and below 7 \% error rate. The error rate and also the rate of completely correctly classified movies did’t improve significantly with increasing number of iterations. Again most improvements in false-positives rates were neutralised by increasing false-negative rates. Only a small improvement between 5 and 20 iterations was notable. The most significant improvements were observable for the 3 difficult genres Drama, Comedy \& Documentary.

\begin{figure}
\begin{center}
\includegraphics[width=3.00in]{ErrorRatesKeywords.png}
\caption{error rates for selected genres using keywords data as the only feature}
\label{fig:ErrorRatesKeywords}
\end{center}
\end{figure}

\subsection{Actors Based}
\par The third experiment (see Figure \ref{fig:ErrorRatesActors})  was to to only use the actors data as the feature vector. This experiment didn’t perform much better than the plot based classification and lead to nearly the same rate of completely correctly classified movies of about 24.8 \%.
\par The error rates stayed nearly constant and also the rate of completely correctly classified movies didn’t change with an increasing number of training iterations. Cause of that this approach delivered better results for a low number of iterations and was faster in the training phase cause of a lower dimensionality of the feature vectors.
As in the previous two experiments the genres Drama, Comedy and Documentary were the ones with the highest error rates.

\begin{figure}
\begin{center}
\includegraphics[width=3.00in]{ActorsErrorRates.png}
\caption{error rates for selected genres using actor data as the only feature}
\label{fig:ErrorRatesActors}
\end{center}
\end{figure}

\subsection{Combined Features}
\par As a last experiment (see Figure \ref{fig:ErrorRatesCombined})  we combined all 3 features and averaged the results to obtain a classification per genre. This lead to significantly better result of up to 43.9 \% completely correctly classified movies. The disadvantage of this approach is the long training phase. For every feature and every genre the systems trains a different SVM, which leads to 72 SVMs that need to be trained.
\par The rate of completely correctly classified movies increased significantly with the number of training iterations. The largest increase is from 5 to 15 iterations - going up from 23.9 \% to 37.3 \% - and after about 45 iterations it only increases a little. Similar to the plot based approach there are some small throwbacks at 20, after 45 and at 70 iterations but overall it still increases.
\par The error rates are a lot better than for the single feature approaches, but still the genres Drama, Comedy and Documentary are outliers compared to the other genres. In this combined approach Drama isn't the worst genre anymore and provides a quite stable overall error rate. Now Comedy is the most difficult genre. The error rate of documentary improved so far that it is now below the average of the other genres. As expected the largest improvement happens within the first 20 iterations. Especially Comedy improves it's error rate from above 20 \% to below 14 \%.
\par Looking at the false-positive and false-negative rates in detail shows some interesting phenomena. The false-negative rates for all genres except Drama (1.5 \% to 5 \%) are really low (< 0.5 \%) and increase only slightly with the increasing number of iterations. The false-positive rates for Drama and Documentary are quite similar or sometimes even below the average of the other genres. But Drama and Comedy have an alternating increase and decrease in the error rates similar to the curve of Drama for the plot based approach. For Drama the alternating pattern smoothes the overall error rate but for Comedy the false-positive rate is orders of magnitude larger than the false-negative rate so that this zick-zack pattern is still visible in the overall error rate.
\par The reason for the zick-zack pattern isn't obvious and would need some more investigation. The results are much better than using just one feature at a time. By now we don't know whether this is due to an actual improved algorithm or just a better quality of the input data since we're only using movies that provide all 3 features and therefore we do not have so many training and testing samples.


\begin{figure}
\begin{center}
\includegraphics[width=3.00in]{ErrorRatesCombined.png}
\caption{error rates when using all 3 features at the same time for classification}
\label{fig:ErrorRatesCombined}
\end{center}
\end{figure}



\section{Conclusion \& Future Work}

\par Overall our implementation couldn't fulfill our goal of an reliable classification system for movie genres. We have seen some improvements by trying different features and especially using feature combinations seemed to work better.
\par The quality of the data is an important factor for machine learning algorithms since bad, insufficient or even misclassified data can ruin the learned model. Having data with more features improves the quality of the prediction and this is one field for future work. Evaluating more features and also try to find out if the higher quality of the prediction with the combined features approach is caused by looking only at movies with a high quality of features or actually by the number and combination of features. One first step in that direction would be to run the single feature approaches on the same subset of the data that the combined approach used.
\par Seeing large error rates for, from a content perspective, very diverse and fuzzy genres like Drama, Comedy \& Documentary makes sense since they contain a large number of movies and even for humans it might be difficult by just looking at the provided features to decide for a genre classification. For other more specific genres like Western or Fantasy the success rates were over 90 \% and based on that one could try to analyse the generated models to derive an answer to the question “What makes up a genre?”.
\par Working with SVMs as machine learning component worked out quite well and was easier to set up than Neural Networks, which we used in other projects. One should note that increasing the number of training iterations usually increases the precision but that doesn't always hold. After all one has to choose between the training time \& cost and the gain in precision. As we've seen with some of the zick-zack curves it is possible to end at a very suboptimal point on the curve.
\par Those zick-zack curves are also a mysterious outcome of our experiments and it would be interesting if this is due to properties of the SVM or of our features and how it behaves in the gaps between our 5 step iteration jumps. Unfortunately we didn't had time to investigate this.
\par Using Apache Spark for our experiments was interesting and it's APIs proved their usefulness. The ML library provided everything we needed and was easy to use. We didn't encounter any bugs and coming from Apache Flink everything was quite self explaining and similar, although they use quite different mechanisms under the hood. We also considered Apache Flink for this project but cause of it's unstable machine learning API, which was also missing an IDF transformer at the beginning of this project, we preferred Spark, which is more battle proven and therefor more useful for a project under heavy time pressure like this.
\par The only thing we couldn't do in neither Spark nor Flink, was the preprocessing of our data. Since it doesn't come in a simple format like CSV and also includes some headers we needed to perform some preprocessing via Python scripts and small Scala programs. 
\par Our dataset wasn't real ‘Big Data’, although we crossed the giga-byte limit all calculations could be performed on a single consumer machine. Training and testing with all 3 features on the whole dataset took less than an hour on a recent quad-core machine with 16 GB of ram. But from an academic point of view it was very interesting to work with those ‘Big Data’ systems and get some hands-on experience with real data.

\section{References}

TODO:
Spark Docu
Book from the Lecture
IMDb

\bigskip
\par The code for this project as well as the raw data results data can be found on https://github.com/FGoessler/AIM3Project.

\end{document}
